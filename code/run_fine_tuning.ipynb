{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules, mainly huggingface basic model handlers.\n",
    "# Make sure you install huggingface and other packages properly.\n",
    "from collections import Counter\n",
    "import json\n",
    "import copy\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from vocab_mismatch_utils import *\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"../huggingface_cache/\" # Not overload common dir \n",
    "                                                           # if run in shared resources.\n",
    "\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from transformers.trainer_utils import is_main_process, EvaluationStrategy\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_args(args, perturbed_type):\n",
    "    \n",
    "    training_args = TrainingArguments(\"tmp_trainer\")\n",
    "    training_args.no_cuda = args.no_cuda\n",
    "    training_args.seed = args.seed\n",
    "    training_args.do_train = args.do_train\n",
    "    training_args.do_eval = args.do_eval\n",
    "\n",
    "    training_args.evaluation_strategy = args.evaluation_strategy # evaluation is done after each epoch\n",
    "    training_args.metric_for_best_model = args.metric_for_best_model\n",
    "    training_args.greater_is_better = args.greater_is_better\n",
    "    training_args.logging_dir = args.logging_dir\n",
    "    training_args.task_name = args.task_name\n",
    "    training_args.learning_rate = args.learning_rate\n",
    "    training_args.per_device_train_batch_size = args.per_device_train_batch_size\n",
    "    training_args.per_device_eval_batch_size = args.per_device_eval_batch_size\n",
    "    training_args.num_train_epochs = args.num_train_epochs # this is the maximum num_train_epochs, we set this to be 100.\n",
    "    training_args.eval_steps = args.eval_steps\n",
    "    training_args.logging_steps = args.logging_steps\n",
    "    training_args.load_best_model_at_end = args.load_best_model_at_end\n",
    "    if args.save_total_limit != -1:\n",
    "        # only set if it is specified\n",
    "        training_args.save_total_limit = args.save_total_limit\n",
    "\n",
    "    training_args.log_level = \"info\"\n",
    "    training_args.log_level_replica = \"info\"\n",
    "    \n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO if is_main_process(training_args.local_rank) else logging.WARN,\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Recieved raw args: {args}\")\n",
    "    \n",
    "    # validation of inputs.\n",
    "    if \"/\" not in args.model_name_or_path:\n",
    "        logger.warning(\"WARNING: you have to use your saved model from mid-tuning, not brand-new models.\")\n",
    "    if \"/\" not in args.tokenizer_name:\n",
    "        logger.warning(\"WARNING: you have to use your saved tokenizer from mid-tuning, not brand-new tokenizers.\")\n",
    "        \n",
    "    logger.info(\"Generating the run name for WANDB for better experiment tracking.\")\n",
    "    import datetime\n",
    "    date_time = \"{}-{}\".format(datetime.datetime.now().month, datetime.datetime.now().day)\n",
    "    \n",
    "    if len(args.model_name_or_path.split(\"/\")) > 1:\n",
    "        run_name = \"{0}_task_{1}_ft_{2}\".format(\n",
    "            date_time,\n",
    "            args.task_name,\n",
    "            \"_\".join(args.model_name_or_path.split(\"/\")[1].split(\"_\")[1:]),\n",
    "        )\n",
    "    else:\n",
    "        if args.no_pretrain:\n",
    "            run_name = \"{0}_task_{1}_ft_{2}_no_pretrain_reinit_emb_{3}_reinit_avg_{4}_token_s_{5}_word_s_{6}_lr_{7}_seed_{8}_reverse_{9}_random_{10}\".format(\n",
    "                date_time,\n",
    "                args.task_name,\n",
    "                args.model_name_or_path,\n",
    "                args.reinit_embeddings,\n",
    "                args.reinit_avg_embeddings,\n",
    "                args.token_swapping,\n",
    "                args.word_swapping,\n",
    "                args.learning_rate,\n",
    "                args.seed,\n",
    "                args.reverse_order,\n",
    "                args.random_order,\n",
    "            )\n",
    "        else:\n",
    "            run_name = \"{0}_task_{1}_ft_{2}_reinit_emb_{3}_reinit_avg_{4}_token_s_{5}_word_s_{6}_lr_{7}_seed_{8}_reverse_{9}_random_{10}\".format(\n",
    "                date_time,\n",
    "                args.task_name,\n",
    "                args.model_name_or_path,\n",
    "                args.reinit_embeddings,\n",
    "                args.reinit_avg_embeddings,\n",
    "                args.token_swapping,\n",
    "                args.word_swapping,\n",
    "                args.learning_rate,\n",
    "                args.seed,\n",
    "                args.reverse_order,\n",
    "                args.random_order,\n",
    "            )\n",
    "    training_args.run_name = run_name\n",
    "    logger.info(f\"WANDB RUN NAME: {training_args.run_name}\")\n",
    "    training_args.output_dir = os.path.join(args.output_dir, run_name)\n",
    "\n",
    "    training_args_dict = training_args.to_dict()\n",
    "    # for PR\n",
    "    _n_gpu = training_args_dict[\"_n_gpu\"]\n",
    "    del training_args_dict[\"_n_gpu\"]\n",
    "    training_args_dict[\"n_gpu\"] = _n_gpu\n",
    "    HfParser = HfArgumentParser((TrainingArguments))\n",
    "    training_args = HfParser.parse_dict(training_args_dict)[0]\n",
    "\n",
    "    if args.model_name_or_path == \"\":\n",
    "        assert False # you have to provide one of them.\n",
    "    # Set seed before initializing model.\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Log on each process the small summary:\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "\n",
    "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
    "    if is_main_process(training_args.local_rank):\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "        transformers.utils.logging.enable_default_handler()\n",
    "        transformers.utils.logging.enable_explicit_format()\n",
    "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "    return training_args\n",
    "\n",
    "def random_corrupt(task, tokenizer, vocab_match, example):\n",
    "    # for tasks that have single sentence\n",
    "    if task == \"sst3\" or task == \"wiki-text\" or task == \"cola\":\n",
    "        original_sentence = example[TASK_CONFIG[task][0]]\n",
    "        if original_sentence != None and original_sentence.strip() != \"\" and original_sentence.strip() != \"None\":\n",
    "            corrupted_sentence = corrupt_translator(original_sentence, tokenizer, vocab_match)\n",
    "            example[TASK_CONFIG[task][0]] = corrupted_sentence\n",
    "    # for tasks that have two sentences\n",
    "    elif task == \"mrpc\" or task == \"mnli\" or task == \"snli\" or task == \"qnli\":\n",
    "        original_sentence = example[TASK_CONFIG[task][0]]\n",
    "        if original_sentence != None and original_sentence.strip() != \"\" and original_sentence.strip() != \"None\":\n",
    "            corrupted_sentence = corrupt_translator(original_sentence, tokenizer, vocab_match)\n",
    "            example[TASK_CONFIG[task][0]] = corrupted_sentence\n",
    "        \n",
    "        original_sentence = example[TASK_CONFIG[task][1]]\n",
    "        if original_sentence != None and original_sentence.strip() != \"\" and original_sentence.strip() != \"None\":\n",
    "            corrupted_sentence = corrupt_translator(original_sentence, tokenizer, vocab_match)\n",
    "            example[TASK_CONFIG[task][1]] = corrupted_sentence\n",
    "    elif task == \"conll2003\" or task == \"en_ewt\":\n",
    "        original_tokens = example[TASK_CONFIG[task][0]]\n",
    "        corrupted_tokens = [vocab_match[t] for t in original_tokens]\n",
    "        example[TASK_CONFIG[task][0]] = corrupted_tokens\n",
    "    else:\n",
    "        print(f\"task={task} not supported yet!\")\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceRoBERTaBase:\n",
    "    \"\"\"\n",
    "    An extension for evaluation based off the huggingface module.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, model, task_name, task_config):\n",
    "        self.task_name = task_name\n",
    "        self.task_config = task_config\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        \n",
    "    def train(self, inoculation_train_df, eval_df, model, args, training_args, max_length=128,\n",
    "              inoculation_patience_count=-1, pd_format=True, \n",
    "              scramble_proportion=0.0, eval_with_scramble=False):\n",
    "\n",
    "        if pd_format:\n",
    "            datasets = {}\n",
    "            datasets[\"train\"] = Dataset.from_pandas(inoculation_train_df)\n",
    "            datasets[\"validation\"] = Dataset.from_pandas(eval_df)\n",
    "        else:\n",
    "            datasets = {}\n",
    "            datasets[\"train\"] = inoculation_train_df\n",
    "            datasets[\"validation\"] = eval_df\n",
    "        logger.info(f\"***** Train Sample Count (Verify): %s *****\"%(len(datasets[\"train\"])))\n",
    "        logger.info(f\"***** Valid Sample Count (Verify): %s *****\"%(len(datasets[\"validation\"])))\n",
    "    \n",
    "        label_list = datasets[\"validation\"].unique(\"label\")\n",
    "        label_list.sort()  # Let's sort it for determinism\n",
    "\n",
    "        sentence1_key, sentence2_key = self.task_config\n",
    "        padding = \"max_length\"\n",
    "        label_to_id = None\n",
    "        def preprocess_function(examples):\n",
    "            # Tokenize the texts\n",
    "            args = (\n",
    "                (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "            )\n",
    "            result = self.tokenizer(*args, padding=padding, max_length=max_length, truncation=True)\n",
    "            # Map labels to IDs (not necessary for GLUE tasks)\n",
    "            if label_to_id is not None and \"label\" in examples:\n",
    "                result[\"label\"] = [label_to_id[l] for l in examples[\"label\"]]\n",
    "            return result\n",
    "        datasets[\"train\"] = datasets[\"train\"].map(preprocess_function, batched=True)\n",
    "        datasets[\"validation\"] = datasets[\"validation\"].map(preprocess_function, batched=True)\n",
    "        \n",
    "        train_dataset = datasets[\"train\"]\n",
    "        eval_dataset = datasets[\"validation\"]\n",
    "        \n",
    "        # Log a few random samples from the training set:\n",
    "        for index in random.sample(range(len(train_dataset)), 3):\n",
    "            logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n",
    "            \n",
    "        metric = load_metric(\"glue\", \"sst2\") # any glue task will do the job, just for eval loss\n",
    "        \n",
    "        def asenti_compute_metrics(p: EvalPrediction):\n",
    "            preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "            result_to_print = classification_report(p.label_ids, preds, digits=5, output_dict=True)\n",
    "            print(classification_report(p.label_ids, preds, digits=5))\n",
    "            mcc_scores = matthews_corrcoef(p.label_ids, preds)\n",
    "            logger.info(f\"MCC scores: {mcc_scores}.\")\n",
    "            result_to_return = metric.compute(predictions=preds, references=p.label_ids)\n",
    "            result_to_return[\"Macro-F1\"] = result_to_print[\"macro avg\"][\"f1-score\"]\n",
    "            result_to_return[\"MCC\"] = mcc_scores\n",
    "            return result_to_return\n",
    "\n",
    "        # Initialize our Trainer. We are only intersted in evaluations\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            compute_metrics=asenti_compute_metrics,\n",
    "            tokenizer=self.tokenizer,\n",
    "            # Data collator will default to DataCollatorWithPadding, so we change it if we already did the padding.\n",
    "            data_collator=default_data_collator\n",
    "        )\n",
    "        # Early stop\n",
    "        if inoculation_patience_count != -1:\n",
    "            trainer.add_callback(EarlyStoppingCallback(inoculation_patience_count))\n",
    "        \n",
    "        # Training\n",
    "        if training_args.do_train:\n",
    "            logger.info(\"*** Training our model ***\")\n",
    "            trainer.train()\n",
    "            trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "        \n",
    "        # Evaluation\n",
    "        eval_results = {}\n",
    "        if training_args.do_eval:\n",
    "            logger.info(\"*** Evaluate ***\")\n",
    "            tasks = [self.task_name]\n",
    "            eval_datasets = [eval_dataset]\n",
    "            for eval_dataset, task in zip(eval_datasets, tasks):\n",
    "                eval_result = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "                output_eval_file = os.path.join(training_args.output_dir, f\"eval_results_{task}.txt\")\n",
    "                if trainer.is_world_process_zero():\n",
    "                    with open(output_eval_file, \"w\") as writer:\n",
    "                        logger.info(f\"***** Eval results {task} *****\")\n",
    "                        for key, value in eval_result.items():\n",
    "                            logger.info(f\"  {key} = {value}\")\n",
    "                            writer.write(f\"{key} = {value}\\n\")\n",
    "                eval_results.update(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    ## Required parameters\n",
    "    parser.add_argument(\"--wandb_proj_name\",\n",
    "                        default=\"\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--task_name\",\n",
    "                        default=\"sst3\",\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--train_file\",\n",
    "                        default=\"../data-files/sst/sst-tenary-train.tsv\",\n",
    "                        type=str,\n",
    "                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "    parser.add_argument(\"--eval_file\",\n",
    "                        default=\"../data-files/sst-tenary/sst-tenary-dev.tsv\",\n",
    "                        type=str,\n",
    "                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "    parser.add_argument(\"--model_name_or_path\",\n",
    "                        default=\"../7-31_roberta-base_roberta-base_seed_42_data_wikitext-15M_inoculation_1.0/\",\n",
    "                        type=str,\n",
    "                        help=\"The pretrained model binary file.\")\n",
    "    parser.add_argument(\"--tokenizer_name\",\n",
    "                        default=\"../7-31_roberta-base_roberta-base_seed_42_data_wikitext-15M_inoculation_1.0/\",\n",
    "                        type=str,\n",
    "                        help=\"Tokenizer name.\")\n",
    "    parser.add_argument(\"--do_train\",\n",
    "                        default=True,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether not to use CUDA when available\")\n",
    "    parser.add_argument(\"--do_eval\",\n",
    "                        default=True,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether not to use CUDA when available\")\n",
    "    parser.add_argument(\"--no_cuda\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether not to use CUDA when available\")\n",
    "    parser.add_argument(\"--evaluation_strategy\",\n",
    "                        default=\"steps\",\n",
    "                        type=str,\n",
    "                        help=\"When you evaluate your training model on eval set.\")\n",
    "    parser.add_argument(\"--cache_dir\",\n",
    "                        default=\"../tmp/\",\n",
    "                        type=str,\n",
    "                        help=\"Cache directory for the evaluation pipeline (not HF cache).\")\n",
    "    parser.add_argument(\"--logging_dir\",\n",
    "                        default=\"../tmp/\",\n",
    "                        type=str,\n",
    "                        help=\"Logging directory.\")\n",
    "    parser.add_argument(\"--output_dir\",\n",
    "                        default=\"../\",\n",
    "                        type=str,\n",
    "                        help=\"Output directory of this training process.\")\n",
    "    parser.add_argument(\"--max_seq_length\",\n",
    "                        default=128,\n",
    "                        type=int,\n",
    "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                             \"than this will be padded.\")\n",
    "    parser.add_argument(\"--learning_rate\",\n",
    "                        default=2e-5,\n",
    "                        type=float,\n",
    "                        help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--seed\",\n",
    "                        default=42,\n",
    "                        type=int,\n",
    "                        help=\"Random seed\")\n",
    "    parser.add_argument(\"--metric_for_best_model\",\n",
    "                        default=\"Macro-F1\",\n",
    "                        type=str,\n",
    "                        help=\"The metric to use to compare two different models.\")\n",
    "    parser.add_argument(\"--greater_is_better\",\n",
    "                        default=True,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether the `metric_for_best_model` should be maximized or not.\")\n",
    "    parser.add_argument(\"--is_tensorboard\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"If tensorboard is connected.\")\n",
    "    parser.add_argument(\"--load_best_model_at_end\",\n",
    "                        default=True,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether load best model and evaluate at the end.\")\n",
    "    parser.add_argument(\"--eval_steps\",\n",
    "                        default=10,\n",
    "                        type=float,\n",
    "                        help=\"The total steps to flush logs to wandb specifically.\")\n",
    "    parser.add_argument(\"--logging_steps\",\n",
    "                        default=10,\n",
    "                        type=float,\n",
    "                        help=\"The total steps to flush logs to wandb specifically.\")\n",
    "    parser.add_argument(\"--save_total_limit\",\n",
    "                        default=1,\n",
    "                        type=int,\n",
    "                        help=\"If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output dir.\")\n",
    "    # these are arguments for inoculations\n",
    "    parser.add_argument(\"--inoculation_patience_count\",\n",
    "                        default=-1,\n",
    "                        type=int,\n",
    "                        help=\"If the evaluation metrics is not increasing with maximum this step number, the training will be stopped.\")\n",
    "    parser.add_argument(\"--per_device_train_batch_size\",\n",
    "                        default=64,\n",
    "                        type=int,\n",
    "                        help=\"\")\n",
    "    parser.add_argument(\"--per_device_eval_batch_size\",\n",
    "                        default=64,\n",
    "                        type=int,\n",
    "                        help=\"\")\n",
    "    parser.add_argument(\"--num_train_epochs\",\n",
    "                        default=3.0,\n",
    "                        type=float,\n",
    "                        help=\"The total number of epochs for training.\")\n",
    "    parser.add_argument(\"--no_pretrain\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to use pretrained model if provided.\")\n",
    "    parser.add_argument(\"--reinit_avg_embeddings\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to reinit embeddings to be the average embeddings.\")\n",
    "    parser.add_argument(\"--reinit_embeddings\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to reinit embeddings to be the random embeddings.\")\n",
    "    parser.add_argument(\"--token_swapping\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to swap token randomly.\")\n",
    "    parser.add_argument(\"--word_swapping\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to swap words randomly.\")\n",
    "    parser.add_argument(\"--swap_vocab_file\",\n",
    "                        default=\"../data-files/wikitext-15M-vocab.json\",\n",
    "                        type=str,\n",
    "                        help=\"Please provide a vocab file if you want to do word swapping.\")\n",
    "    parser.add_argument(\"--train_embeddings_only\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"If only train embeddings not the whole model.\")\n",
    "    parser.add_argument(\"--train_linear_layer_only\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"If only train embeddings not the whole model.\")\n",
    "    # these are arguments for scrambling texts\n",
    "    parser.add_argument(\"--reverse_order\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to reverse the sequence order.\")\n",
    "    parser.add_argument(\"--random_order\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"Whether to random order the sequence.\")\n",
    "    parser.add_argument(\"--scramble_proportion\",\n",
    "                        default=0.0,\n",
    "                        type=float,\n",
    "                        help=\"What is the percentage of text you want to scramble.\")\n",
    "    parser.add_argument(\"--inoculation_p\",\n",
    "                        default=1.0,\n",
    "                        type=float,\n",
    "                        help=\"How many data you need to train\")\n",
    "    parser.add_argument(\"--eval_with_scramble\",\n",
    "                        default=False,\n",
    "                        action='store_true',\n",
    "                        help=\"If you are also evaluating with scrambled texts.\")\n",
    "    parser.add_argument(\"--n_layer_to_finetune\",\n",
    "                        default=-1,\n",
    "                        type=int,\n",
    "                        help=\"Indicate a number that is less than original layer if you only want to finetune with earlier layers only.\")\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    # os.environ[\"WANDB_DISABLED\"] = \"NO\" if args.is_tensorboard else \"YES\" # BUG\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = \"../huggingface_inoculation_cache/\"\n",
    "    os.environ[\"WANDB_PROJECT\"] = f\"fine_tuning\"\n",
    "    \n",
    "    # if cache does not exist, create one\n",
    "    if not os.path.exists(os.environ[\"TRANSFORMERS_CACHE\"]): \n",
    "        os.makedirs(os.environ[\"TRANSFORMERS_CACHE\"])\n",
    "    TASK_CONFIG = {\n",
    "        \"sst3\": (\"text\", None),\n",
    "        \"cola\": (\"sentence\", None),\n",
    "        \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "        \"snli\": (\"premise\", \"hypothesis\"),\n",
    "        \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "        \"qnli\": (\"question\", \"sentence\")\n",
    "    }\n",
    "    # Load pretrained model and tokenizer\n",
    "    NUM_LABELS = 2 if args.task_name == \"cola\" or args.task_name == \"mrpc\" or args.task_name == \"qnli\" else 3\n",
    "    MAX_SEQ_LEN = args.max_seq_length\n",
    "    \n",
    "    args.tokenizer_name = args.model_name_or_path\n",
    "    name_list = args.model_name_or_path.split(\"_\")\n",
    "    \n",
    "    perturbed_type = \"\"\n",
    "    \n",
    "    inoculation_p = 0.0\n",
    "    if not args.no_pretrain:\n",
    "        for i in range(len(name_list)):\n",
    "            if name_list[i] == \"seed\":\n",
    "                args.seed = int(name_list[i+1])\n",
    "            if name_list[i] == \"reverse\":\n",
    "                if name_list[i+1] == \"True\":\n",
    "                    args.reverse_order = True\n",
    "                else:\n",
    "                    args.reverse_order = False\n",
    "            if name_list[i] == \"random\":\n",
    "                if name_list[i+1].strip(\"/\") == \"True\":\n",
    "                    args.random_order = True\n",
    "                else:\n",
    "                    args.random_order = False\n",
    "            if name_list[i] == \"data\":\n",
    "                if len(name_list[i+1].split(\"-\")) > 2:\n",
    "                    perturbed_type = \"-\".join(name_list[i+1].split(\"-\")[2:])\n",
    "            if name_list[i] == \"inoculation\":\n",
    "                inoculation_p = float(name_list[i+1])\n",
    "    if \"word_s_True\" in args.model_name_or_path:\n",
    "        args.word_swapping = True\n",
    "        \n",
    "    if perturbed_type == \"\":\n",
    "        args.train_file = f\"../data-files/{args.task_name}\"\n",
    "    else:\n",
    "        args.train_file = f\"../data-files/{args.task_name}-{perturbed_type}\"\n",
    "    \n",
    "    training_args = generate_training_args(args, perturbed_type)\n",
    "    \n",
    "    need_resize = False\n",
    "    if inoculation_p == 0.0:\n",
    "        logger.warning(f\"***** WARNING: Detected inoculation_p={inoculation_p}; initialize the model and the tokenizer from huggingface. *****\")\n",
    "        # we need to make sure tokenizer is the correct one!\n",
    "        if \"albert-base-v2\" in args.model_name_or_path:\n",
    "            args.tokenizer_name = \"albert-base-v2\"\n",
    "            need_resize = True\n",
    "        elif \"bert-base-cased\" in args.model_name_or_path:\n",
    "            args.tokenizer_name = \"bert-base-cased\"\n",
    "            need_resize = True\n",
    "        else:\n",
    "            args.tokenizer_name = \"roberta-base\"\n",
    "        args.model_name_or_path = \"roberta-base\"\n",
    "        \n",
    "    config = AutoConfig.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        num_labels=NUM_LABELS,\n",
    "        finetuning_task=args.task_name,\n",
    "        cache_dir=args.cache_dir\n",
    "    )\n",
    "    \n",
    "    if need_resize:\n",
    "        # we need to rewrite the number of type token a little\n",
    "        # during pretraining, there are two types for reberta\n",
    "        # during fine-tuning, i think we are only using one?\n",
    "        if os.path.isdir(args.model_name_or_path):\n",
    "            if args.tokenizer_name == \"albert-base-v2\":\n",
    "                config.type_vocab_size = 1\n",
    "            else:\n",
    "                config.type_vocab_size = 2\n",
    "        else:\n",
    "            if args.tokenizer_name == \"albert-base-v2\":\n",
    "                config.type_vocab_size = 1\n",
    "            else:\n",
    "                config.type_vocab_size = 1\n",
    "\n",
    "    if args.n_layer_to_finetune != -1:\n",
    "        # then we are only finetuning n-th layer, not all the layers\n",
    "        if args.n_layer_to_finetune > config.num_hidden_layers:\n",
    "            logger.info(f\"***** WARNING: You are trying to train with first {args.n_layer_to_finetune} layers only *****\")\n",
    "            logger.info(f\"***** WARNING: But the model has only {config.num_hidden_layers} layers *****\")\n",
    "            logger.info(f\"***** WARNING: Training with all layers instead! *****\")\n",
    "            pass # just to let it happen, just train it with all layers\n",
    "        else:\n",
    "            # overwrite\n",
    "            logger.info(f\"***** WARNING: You are trying to train with first {args.n_layer_to_finetune} layers only *****\")\n",
    "            logger.info(f\"***** WARNING: But the model has only {config.num_hidden_layers} layers *****\")\n",
    "            config.num_hidden_layers = args.n_layer_to_finetune\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.tokenizer_name,\n",
    "        use_fast=False,\n",
    "        cache_dir=args.cache_dir\n",
    "    )\n",
    "    \n",
    "    if args.no_pretrain:\n",
    "        logger.info(\"***** Training new model from scratch *****\")\n",
    "        model = AutoModelForSequenceClassification.from_config(config)\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            from_tf=False,\n",
    "            config=config,\n",
    "            cache_dir=args.cache_dir\n",
    "        )\n",
    "\n",
    "    if need_resize:\n",
    "        logger.info(\"***** Replacing the word_embeddings and token_type_embeddings with random initialized values *****\")\n",
    "        # this means, we are finetuning directly with new tokenizer.\n",
    "        # so the model itself has a different tokenizer, we need to resize.\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        # If we resize, we also enforce it to reinit\n",
    "        # so we are controlling for weights distribution.\n",
    "        random_config = AutoConfig.from_pretrained(\n",
    "            args.model_name_or_path, \n",
    "            num_labels=NUM_LABELS,\n",
    "            finetuning_task=args.task_name,\n",
    "            cache_dir=args.cache_dir\n",
    "        )\n",
    "        # we need to check if type embedding need to be resized as well.\n",
    "        tokenizer_config = AutoConfig.from_pretrained(\n",
    "            args.tokenizer_name, \n",
    "            num_labels=NUM_LABELS,\n",
    "            finetuning_task=args.task_name,\n",
    "            cache_dir=args.cache_dir\n",
    "        )\n",
    "        # IMPORTANT: THIS ENSURES TYPE WILL NOT CAUSE UNREF POINTER ISSUE.\n",
    "        random_config.type_vocab_size = tokenizer_config.type_vocab_size\n",
    "        random_model = AutoModelForSequenceClassification.from_config(\n",
    "            config=random_config,\n",
    "        )\n",
    "        random_model.resize_token_embeddings(len(tokenizer))\n",
    "        replacing_embeddings = random_model.roberta.embeddings.word_embeddings.weight.data.clone()\n",
    "        model.roberta.embeddings.word_embeddings.weight.data = replacing_embeddings\n",
    "        replacing_type_embeddings = random_model.roberta.embeddings.token_type_embeddings.weight.data.clone()\n",
    "        model.roberta.embeddings.token_type_embeddings.weight.data = replacing_type_embeddings\n",
    "    \n",
    "    if args.reinit_avg_embeddings:\n",
    "        logger.info(\"***** WARNING: We reinit all embeddings to be the average embedding from the pretrained model. *****\")\n",
    "        pretrained_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            from_tf=False,\n",
    "            config=config,\n",
    "            cache_dir=args.cache_dir\n",
    "        )\n",
    "        avg_embeddings = torch.mean(pretrained_model.roberta.embeddings.word_embeddings.weight.data, dim=0).expand_as(model.roberta.embeddings.word_embeddings.weight.data)\n",
    "        model.roberta.embeddings.word_embeddings.weight.data = avg_embeddings\n",
    "        # to keep consistent, we also need to reinit the type embeddings.\n",
    "        random_model = AutoModelForSequenceClassification.from_config(\n",
    "            config=config,\n",
    "        )\n",
    "        replacing_type_embeddings = random_model.roberta.embeddings.token_type_embeddings.weight.data.clone()\n",
    "        model.roberta.embeddings.token_type_embeddings.weight.data = replacing_type_embeddings\n",
    "    elif args.reinit_embeddings:\n",
    "        logger.info(\"***** WARNING: We reinit all embeddings to be the randomly initialized embeddings. *****\")\n",
    "        random_model = AutoModelForSequenceClassification.from_config(config)\n",
    "        # random_model.resize_token_embeddings(len(tokenizer))\n",
    "        replacing_embeddings = random_model.roberta.embeddings.word_embeddings.weight.data.clone()\n",
    "        model.roberta.embeddings.word_embeddings.weight.data = replacing_embeddings\n",
    "        # to keep consistent, we also need to reinit the type embeddings.\n",
    "        random_model = AutoModelForSequenceClassification.from_config(\n",
    "            config=config,\n",
    "        )\n",
    "        replacing_type_embeddings = random_model.roberta.embeddings.token_type_embeddings.weight.data.clone()\n",
    "        model.roberta.embeddings.token_type_embeddings.weight.data = replacing_type_embeddings\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    if args.token_swapping:\n",
    "        logger.info(\"***** WARNING: We are swapping tokens via embeddings. *****\")\n",
    "        original_embeddings = model.roberta.embeddings.word_embeddings.weight.data.clone()\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(args.seed)\n",
    "        perm_idx = torch.randperm(original_embeddings.size()[0], generator=g)\n",
    "        swapped_embeddings = original_embeddings.index_select(dim=0, index=perm_idx)\n",
    "        model.roberta.embeddings.word_embeddings.weight.data = swapped_embeddings\n",
    "    elif args.word_swapping:\n",
    "        logger.info(\"***** WARNING: We are swapping words in the inputs. *****\")\n",
    "        token_frequency_map = json.load(open(args.swap_vocab_file))\n",
    "        wikitext_vocab = list(set(token_frequency_map.keys()))\n",
    "        wikitext_vocab_copy = copy.deepcopy(wikitext_vocab)\n",
    "        random.Random(args.seed).shuffle(wikitext_vocab_copy)\n",
    "        word_swap_map = {}\n",
    "        for i in range(len(wikitext_vocab)):\n",
    "            word_swap_map[wikitext_vocab[i]] = wikitext_vocab_copy[i]\n",
    "    \n",
    "    assert len(tokenizer) == model.roberta.embeddings.word_embeddings.weight.data.shape[0]\n",
    "    \n",
    "    logger.info(f\"***** Current setups *****\")\n",
    "    logger.info(f\"***** model type: {args.model_name_or_path} *****\")\n",
    "    logger.info(f\"***** tokenizer type: {args.tokenizer_name} *****\")\n",
    "    \n",
    "    # We cannot resize this. In the mid-tuning, this is already resized.\n",
    "    # if args.tokenizer_name != args.model_name_or_path:\n",
    "    #     model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "    if args.train_embeddings_only:\n",
    "        logger.info(\"***** We only train embeddings, not other layers *****\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'word_embeddings' not in name: # only word embeddings\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    if args.train_linear_layer_only:\n",
    "        logger.info(\"***** We only train classifier head, not other layers *****\")\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'classifier' not in name: # only word embeddings\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    train_pipeline = HuggingFaceRoBERTaBase(tokenizer, \n",
    "                                            model, args.task_name, \n",
    "                                            TASK_CONFIG[args.task_name])\n",
    "    logger.info(f\"***** TASK NAME: {args.task_name} *****\")\n",
    "    \n",
    "    # we use panda loader now, to make sure it is backward compatible\n",
    "    # with our file writer.\n",
    "    pd_format = False\n",
    "    logger.info(f\"***** Loading pre-loaded datasets from the disk directly! *****\")\n",
    "    datasets = DatasetDict.load_from_disk(args.train_file)\n",
    "\n",
    "    def reverse_order(example):\n",
    "        fields = TASK_CONFIG[args.task_name]\n",
    "        for field in fields:\n",
    "            if field:\n",
    "                original_text = example[field]\n",
    "                original_text = original_text.split(\" \")[::-1]\n",
    "                example[field] = \" \".join(original_text)\n",
    "        return example\n",
    "\n",
    "    def random_order(example):\n",
    "        fields = TASK_CONFIG[args.task_name]\n",
    "        for field in fields:\n",
    "            if field:\n",
    "                original_text = example[field]\n",
    "                original_text = original_text.split(\" \")\n",
    "                random.shuffle(original_text)\n",
    "                example[field] = \" \".join(original_text)\n",
    "        return example\n",
    "\n",
    "    if args.reverse_order:\n",
    "        logger.warning(\"WARNING: you are reversing the order of your sequences.\")\n",
    "        datasets[\"train\"] = datasets[\"train\"].map(reverse_order)\n",
    "        datasets[\"validation\"] = datasets[\"validation\"].map(reverse_order)\n",
    "        datasets[\"test\"] = datasets[\"test\"].map(reverse_order)\n",
    "\n",
    "    if args.random_order:\n",
    "        logger.warning(\"WARNING: you are random ordering your sequences.\")\n",
    "        datasets[\"train\"] = datasets[\"train\"].map(random_order)\n",
    "        datasets[\"validation\"] = datasets[\"validation\"].map(random_order)\n",
    "        datasets[\"test\"] = datasets[\"test\"].map(random_order)\n",
    "    # we don't care about test set in this script?\n",
    "\n",
    "    if args.word_swapping:\n",
    "        logger.warning(\"WARNING: performing word swapping.\")\n",
    "        # we need to do the swap on the data files.\n",
    "        # this tokenizer helps you to get piece length for each token\n",
    "        modified_tokenizer = ModifiedBertTokenizer(\n",
    "            vocab_file=\"../data-files/bert_vocab.txt\")\n",
    "        modified_basic_tokenizer = ModifiedBasicTokenizer()\n",
    "        datasets[\"train\"] = datasets[\"train\"].map(partial(random_corrupt, \n",
    "                                                       args.task_name,\n",
    "                                                       modified_basic_tokenizer, \n",
    "                                                       word_swap_map))\n",
    "        datasets[\"validation\"] = datasets[\"validation\"].map(partial(random_corrupt, \n",
    "                                                       args.task_name,\n",
    "                                                       modified_basic_tokenizer, \n",
    "                                                       word_swap_map))\n",
    "        datasets[\"test\"] = datasets[\"test\"].map(partial(random_corrupt, \n",
    "                                                       args.task_name,\n",
    "                                                       modified_basic_tokenizer, \n",
    "                                                       word_swap_map))\n",
    "\n",
    "    # this may not always start for zero inoculation\n",
    "    datasets[\"train\"] = datasets[\"train\"].shuffle(seed=args.seed)\n",
    "    inoculation_train_df = datasets[\"train\"]\n",
    "    eval_df = datasets[\"validation\"]\n",
    "    # datasets[\"validation\"] = datasets[\"validation\"].shuffle(seed=args.seed)\n",
    "\n",
    "    train_pipeline.train(inoculation_train_df, eval_df, \n",
    "                         model, args,\n",
    "                         training_args, max_length=args.max_seq_length,\n",
    "                         inoculation_patience_count=args.inoculation_patience_count, pd_format=pd_format, \n",
    "                         scramble_proportion=args.scramble_proportion, eval_with_scramble=args.eval_with_scramble)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
