{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UPDATES:\n",
    "When we read in long paragraphs which contain multiple sentences,\n",
    "we will need Stanza to chunk them into sentences. This causes some\n",
    "problems when generating conullu files where a \"sentence\" will be\n",
    "essentially chunked into multiple sentences, and we need to merge\n",
    "sentences back when generating the perturbed datasets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import stanza\n",
    "from stanza.utils.conll import CoNLL\n",
    "import os\n",
    "import argparse\n",
    "from datasets import DatasetDict\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from datasets import list_datasets\n",
    "import logging\n",
    "import pathlib\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def get_sentence_doc(sentence_in):\n",
    "    doc = nlp(sentence_in)\n",
    "    return doc\n",
    "\n",
    "def get_postag_token(sentence_in):\n",
    "    ret = []\n",
    "    doc = nlp(sentence_in)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            ret  += [(word.text, word.upos, word.xpos,)]\n",
    "    return ret\n",
    "\n",
    "def partition(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parse():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='pos-tagging config.')\n",
    "    # Experiment management:\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=128,\n",
    "                        help='Training batch size.')\n",
    "    parser.add_argument('--max_number_of_examples', type=int, default=-1,\n",
    "                        help='Max number of examples to load for each splits.')\n",
    "    parser.add_argument('--data_dir', type=str, default=\"../../data-files/wikitext-15M/\",\n",
    "                        help='Whether to resume for this file.')\n",
    "    parser.add_argument('--output_dir', type=str, default=\"../../data-files/wikitext-15M-conllu/\",\n",
    "                        help='Whether to resume for this file.')\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                        help='Random seed.')\n",
    "    parser.add_argument(\"--include_train\",\n",
    "                       default=False,\n",
    "                       action='store_true',\n",
    "                       help=\"Whether to include train.\")  \n",
    "    parser.add_argument(\"--include_test\",\n",
    "                       default=False,\n",
    "                       action='store_true',\n",
    "                       help=\"Whether to include test.\")  \n",
    "    parser.add_argument(\"--include_validation\",\n",
    "                       default=False,\n",
    "                       action='store_true',\n",
    "                       help=\"Whether to run eval on the test set.\")  \n",
    "    parser.set_defaults(\n",
    "        # Exp management:\n",
    "        seed=42,\n",
    "    )\n",
    "    try:\n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        args = parser.parse_args([])\n",
    "    except:\n",
    "        args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Loading arguments\n",
    "    args = arg_parse()\n",
    "    try:        \n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        # Experiment management:\n",
    "        args.batch_size=128\n",
    "        args.data_dir=\"../../data-files/wikitext-15M/\"\n",
    "        args.output_dir=\"../../data-files/wikitext-15M-conllu/\"\n",
    "        args.seed=42\n",
    "        is_jupyter = True\n",
    "    except:\n",
    "        is_jupyter = False\n",
    "        \n",
    "    if not args.include_train and not args.include_test and not args.include_validation:\n",
    "        logging.error(\"Need to at least specify a single partition.\")\n",
    "        assert False\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        \n",
    "    # Create output directory if not exists.\n",
    "    pathlib.Path(args.output_dir).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, \n",
    "        format='%(asctime)s %(levelname)-8s %(message)s', \n",
    "        datefmt='%a, %d %b %Y %H:%M:%S', \n",
    "        filename=os.path.join(args.output_dir, \"training.log\"),\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(os.sys.stdout))\n",
    "    \n",
    "    logging.info(\"Args:\")\n",
    "    logging.info(args)\n",
    "    \n",
    "    logging.info(\"Running conllu transformation with data lives in:\")\n",
    "    logging.info(args.data_dir)\n",
    "    wiki_datasets = DatasetDict.load_from_disk(args.data_dir)\n",
    "    \n",
    "    logging.info(\"Removing any existing files including:\")\n",
    "    # output file cleanup if exist.\n",
    "    train_output_file = os.path.join(args.output_dir, \"wikitext-15M-train.conllu\")\n",
    "    test_output_file = os.path.join(args.output_dir, \"wikitext-15M-test.conllu\")\n",
    "    validation_output_file = os.path.join(args.output_dir, \"wikitext-15M-validation.conllu\")\n",
    "    train_json_file = os.path.join(args.output_dir, \"wikitext-15M-train.json\")\n",
    "    test_json_file = os.path.join(args.output_dir, \"wikitext-15M-test.json\")\n",
    "    validation_json_file = os.path.join(args.output_dir, \"wikitext-15M-validation.json\")\n",
    "    logging.info(train_output_file)\n",
    "    logging.info(validation_output_file)\n",
    "    logging.info(test_output_file)\n",
    "    logging.info(train_json_file)\n",
    "    logging.info(test_json_file)\n",
    "    logging.info(validation_json_file)\n",
    "    try:\n",
    "        os.remove(train_output_file)\n",
    "        os.remove(test_output_file)\n",
    "        os.remove(validation_output_file)\n",
    "        os.remove(train_json_file)\n",
    "        os.remove(test_json_file)\n",
    "        os.remove(validation_json_file)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    # we are appending so later scripts can process this file in batch?\n",
    "    write_mode = \"a+\"\n",
    "    \n",
    "    # Stanza\n",
    "    # We need to set tokenize_no_ssplit to False, and we need to merge at later stage\n",
    "    # so that our shifting is NOT across sentences which would not make sense for long\n",
    "    # sequences in the wiki-text data, for instance.\n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,pos', tokenize_no_ssplit=False)\n",
    "    logging.info(\"Finish loading Stanza in.\")\n",
    "    \n",
    "    def preprocess(wiki_datasets, args, split):\n",
    "        \n",
    "        logging.info(f\"Preprocessing split={split}.\")\n",
    "        sentences = []\n",
    "        count = 0\n",
    "        for s in wiki_datasets[split]:\n",
    "            if len(s[\"text\"].strip()) > 0:\n",
    "                clean_s = []\n",
    "                for t in s[\"text\"].strip().split(\" \"):\n",
    "                    if len(t.strip()) > 0:\n",
    "                        clean_s += [t.strip()]\n",
    "                sentences += [\" \".join(clean_s)] # we strip it, and split by space.\n",
    "                count += 1\n",
    "                if count == args.max_number_of_examples:\n",
    "                    break\n",
    "        \n",
    "        chunks = list(partition(sentences, args.batch_size))\n",
    "        total_chunk = len(chunks)\n",
    "        count = 0\n",
    "        \n",
    "        if split == \"train\":\n",
    "            output_file = train_output_file\n",
    "            output_json = train_json_file\n",
    "        elif split == \"test\":\n",
    "            output_file = test_output_file\n",
    "            output_json = test_json_file\n",
    "        else:\n",
    "            output_file = validation_output_file\n",
    "            output_json = validation_json_file\n",
    "        \n",
    "        sentence_group = []\n",
    "        for chunk in chunks:\n",
    "            logging.info(f\"processing: {count+1}/{total_chunk}.\")\n",
    "            in_docs = [stanza.Document([], text=d) for d in chunk]\n",
    "            docs = nlp(in_docs)\n",
    "            for i in range(len(docs)):\n",
    "                # count the number of sentences, and we need to save it somewhere\n",
    "                # so that we can merge them back at the end.\n",
    "                sentence_count = len(docs[i].sentences)\n",
    "                sentence_group += [sentence_count]\n",
    "                CoNLL.write_doc2conll(docs[i], output_file, mode=write_mode)\n",
    "            count += 1\n",
    "        logging.info(\"Saving sentence slicing information into:\")\n",
    "        logging.info(output_json)\n",
    "        # dump to the disk.\n",
    "        with open(output_json, \"w\") as fd:\n",
    "            json.dump(sentence_group, fd, indent=4)\n",
    "            \n",
    "    if args.include_train:\n",
    "        preprocess(wiki_datasets, args, \"train\")\n",
    "    if args.include_validation:\n",
    "        preprocess(wiki_datasets, args, \"validation\")\n",
    "    if args.include_test:\n",
    "        preprocess(wiki_datasets, args, \"test\")\n",
    "    \n",
    "    logging.info(\"Saved Pos-tagging with data to:\")\n",
    "    logging.info(args.output_dir)\n",
    "    \n",
    "    logging.info(f\"Finish.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
