{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UPDATES:\n",
    "When we read in long paragraphs which contain multiple sentences,\n",
    "we will need Stanza to chunk them into sentences. This causes some\n",
    "problems when generating conullu files where a \"sentence\" will be\n",
    "essentially chunked into multiple sentences, and we need to merge\n",
    "sentences back when generating the perturbed datasets.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import stanza\n",
    "from stanza.utils.conll import CoNLL\n",
    "import os\n",
    "import argparse\n",
    "from datasets import DatasetDict\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from datasets import list_datasets\n",
    "import logging\n",
    "import pathlib\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "\n",
    "text_fields_map = {\n",
    "    \"wikitext-15M\":\"text\",\n",
    "    \"sst3\":\"text\",\n",
    "    \"snli\":\"premise,hypothesis\",\n",
    "    \"mrpc\":\"sentence1,sentence2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetDict.load_from_disk(\"../../data-files/sst3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Everything that has to do with Yvan and Charlotte, and everything that has to do with Yvan's rambunctious, Jewish sister and her non-Jew husband, feels funny and true.\",\n",
       " 'label': 1,\n",
       " 'source': 'sst'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def get_sentence_doc(sentence_in):\n",
    "    doc = nlp(sentence_in)\n",
    "    return doc\n",
    "\n",
    "def get_postag_token(sentence_in):\n",
    "    ret = []\n",
    "    doc = nlp(sentence_in)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            ret  += [(word.text, word.upos, word.xpos,)]\n",
    "    return ret\n",
    "\n",
    "def partition(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parse():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='pos-tagging config.')\n",
    "    # Experiment management:\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=128,\n",
    "                        help='Training batch size.')\n",
    "    parser.add_argument('--max_number_of_examples', type=int, default=-1,\n",
    "                        help='Max number of examples to load for each splits.')\n",
    "    parser.add_argument('--data_dir', type=str, default=\"../../data-files/\",\n",
    "                        help='Whether to resume for this file.')\n",
    "    parser.add_argument('--task', type=str, default=\"sst3\",\n",
    "                        help='Whether to resume for this file.')\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                        help='Random seed.')\n",
    "    parser.add_argument(\"--include_train\",\n",
    "                       default=False,\n",
    "                       action='store_true',\n",
    "                       help=\"Whether to include train.\")  \n",
    "    parser.add_argument(\"--include_test\",\n",
    "                       default=False,\n",
    "                       action='store_true',\n",
    "                       help=\"Whether to include test.\")  \n",
    "    parser.add_argument(\"--include_validation\",\n",
    "                       default=False,\n",
    "                       action='store_true',\n",
    "                       help=\"Whether to run eval on the test set.\")  \n",
    "    parser.add_argument(\"--include_all_feilds\",\n",
    "                       default=True,\n",
    "                       action='store_true',\n",
    "                       help=\"Whether to include all fields.\")  \n",
    "    parser.set_defaults(\n",
    "        # Exp management:\n",
    "        seed=42,\n",
    "    )\n",
    "    try:\n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        args = parser.parse_args([])\n",
    "    except:\n",
    "        args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Loading arguments\n",
    "    args = arg_parse()\n",
    "    try:        \n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        # Experiment management:\n",
    "        args.batch_size=128\n",
    "        args.data_dir=\"../../data-files/\"\n",
    "        args.task=\"sst3\"\n",
    "        args.seed=42\n",
    "        include_all_feilds = True\n",
    "        is_jupyter = True\n",
    "    except:\n",
    "        is_jupyter = False\n",
    "        \n",
    "    if not args.include_train and not args.include_test and not args.include_validation:\n",
    "        logging.error(\"Need to at least specify a single partition.\")\n",
    "        assert False\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        \n",
    "    # Create output directory if not exists.\n",
    "    args.output_dir = os.path.join(args.data_dir, f\"{args.task}-conllu\")\n",
    "    args.data_dir = os.path.join(args.data_dir, args.task)\n",
    "    pathlib.Path(args.output_dir).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, \n",
    "        format='%(asctime)s %(levelname)-8s %(message)s', \n",
    "        datefmt='%a, %d %b %Y %H:%M:%S', \n",
    "        filename=os.path.join(args.output_dir, \"training.log\"),\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(os.sys.stdout))\n",
    "    \n",
    "    logging.info(\"Args:\")\n",
    "    logging.info(args)\n",
    "    \n",
    "    logging.info(\"Running conllu transformation with data lives in:\")\n",
    "    logging.info(args.data_dir)\n",
    "    datasets = DatasetDict.load_from_disk(args.data_dir)\n",
    "    \n",
    "    if args.task not in text_fields_map:\n",
    "        logging.error(\"You task is not supported by this script: \", args.task)\n",
    "        \n",
    "    text_field = text_fields_map[args.task].split(\",\")\n",
    "    if len(text_field) > 1:\n",
    "        logging.info(\"This dataset contains multiple text fields to shift: \", text_field)\n",
    "    \n",
    "    # collecing all other fields besides the text field.\n",
    "    other_fields = []\n",
    "    for f in datasets[\"train\"][0].keys():\n",
    "        if f not in text_field:\n",
    "            other_fields += [f]\n",
    "    logging.info(\"You are also including these metadata in your data: \", other_fields)\n",
    "    \n",
    "    logging.info(\"Removing any existing files including:\")\n",
    "    # output file cleanup if exist.\n",
    "    train_output_file = os.path.join(args.output_dir, f\"{args.task}-train\")\n",
    "    test_output_file = os.path.join(args.output_dir, f\"{args.task}-test\")\n",
    "    validation_output_file = os.path.join(args.output_dir, f\"{args.task}-validation\")\n",
    "    train_json_file = os.path.join(args.output_dir, f\"{args.task}-train.json\")\n",
    "    test_json_file = os.path.join(args.output_dir, f\"{args.task}-test.json\")\n",
    "    validation_json_file = os.path.join(args.output_dir, f\"{args.task}-validation.json\")\n",
    "    logging.info(train_output_file)\n",
    "    logging.info(validation_output_file)\n",
    "    logging.info(test_output_file)\n",
    "    logging.info(train_json_file)\n",
    "    logging.info(test_json_file)\n",
    "    logging.info(validation_json_file)\n",
    "    try:\n",
    "        for text_f in text_field:\n",
    "            os.remove(train_output_file+f\"-{text_f}.conllu\")\n",
    "            os.remove(test_output_file+f\"-{text_f}.conllu\")\n",
    "            os.remove(validation_output_file+f\"-{text_f}.conllu\")\n",
    "        os.remove(train_json_file)\n",
    "        os.remove(test_json_file)\n",
    "        os.remove(validation_json_file)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    # we are appending so later scripts can process this file in batch?\n",
    "    write_mode = \"a+\"\n",
    "    \n",
    "    # Stanza\n",
    "    # We need to set tokenize_no_ssplit to False, and we need to merge at later stage\n",
    "    # so that our shifting is NOT across sentences which would not make sense for long\n",
    "    # sequences in the wiki-text data, for instance.\n",
    "    nlp = stanza.Pipeline(lang='en', processors='tokenize,pos', tokenize_no_ssplit=False)\n",
    "    logging.info(\"Finish loading Stanza in.\")\n",
    "    \n",
    "    def preprocess(datasets, args, split):\n",
    "        \n",
    "        logging.info(f\"Preprocessing split={split}.\")\n",
    "        sentences = []\n",
    "        fields = []\n",
    "        count = 0\n",
    "        for s in datasets[split]:\n",
    "            clean_ss = []\n",
    "            for text in text_field:\n",
    "                if s[text] != None and len(s[text].strip()) > 0:\n",
    "                    clean_s = []\n",
    "                    for t in s[text].strip().split(\" \"):\n",
    "                        if len(t.strip()) > 0:\n",
    "                            clean_s += [t.strip()]\n",
    "                    clean_ss += [\" \".join(clean_s)]\n",
    "                else:\n",
    "                    clean_ss += [\"\"]\n",
    "                    \n",
    "            # we only allow where all fields are valid.\n",
    "            if \"\" not in clean_ss:\n",
    "                sentences += [dict(zip(text_field, clean_ss))]\n",
    "                _fields = []\n",
    "                for f in other_fields:\n",
    "                    _fields += [s[f]]\n",
    "                fields += [_fields]\n",
    "                count += 1\n",
    "                if count == args.max_number_of_examples:\n",
    "                    break\n",
    "        \n",
    "        assert len(sentences) == len(fields), f\"sentence count {len(sentences)} is not equal to fields count {len(fields)}\"\n",
    "        \n",
    "        chunks = list(partition(sentences, args.batch_size))\n",
    "        total_chunk = len(chunks)\n",
    "        count = 0\n",
    "        \n",
    "        if split == \"train\":\n",
    "            output_file = train_output_file\n",
    "            output_json = train_json_file\n",
    "        elif split == \"test\":\n",
    "            output_file = test_output_file\n",
    "            output_json = test_json_file\n",
    "        else:\n",
    "            output_file = validation_output_file\n",
    "            output_json = validation_json_file\n",
    "        \n",
    "        all_meta = {}\n",
    "        for text_f in text_field:\n",
    "            all_meta[text_f] = []\n",
    "            # this is for corner case where different fields will have\n",
    "            # different metadata for example conllu object counts.\n",
    "            \n",
    "        idx = 0\n",
    "        for chunk in chunks:\n",
    "            logging.info(f\"processing: {count+1}/{total_chunk}.\")\n",
    "            # we need to also take care the multi text fields cases\n",
    "            for text_f in text_field:\n",
    "                in_docs = [stanza.Document([], text=d[text_f]) for d in chunk]\n",
    "                docs = nlp(in_docs)\n",
    "                for i in range(len(docs)):\n",
    "                    CoNLL.write_doc2conll(docs[i], output_file+f\"-{text_f}.conllu\", mode=write_mode)\n",
    "                    # count the number of sentences, and we need to save it somewhere\n",
    "                    # so that we can merge them back at the end.\n",
    "                    sentence_count = len(docs[i].sentences)\n",
    "                    meta = copy.deepcopy(fields[idx+i])\n",
    "                    meta += [sentence_count]\n",
    "                    all_meta[text_f] += [meta]\n",
    "\n",
    "            idx += len(chunk)\n",
    "            count += 1\n",
    "        logging.info(\"Saving sentence slicing information and metadata (other fields) into:\")\n",
    "        logging.info(output_json)\n",
    "        # dump to the disk.\n",
    "        with open(output_json, \"w\") as fd:\n",
    "            json.dump(all_meta, fd, indent=4)\n",
    "            \n",
    "    if args.include_train:\n",
    "        preprocess(datasets, args, \"train\")\n",
    "    if args.include_validation:\n",
    "        preprocess(datasets, args, \"validation\")\n",
    "    if args.include_test:\n",
    "        preprocess(datasets, args, \"test\")\n",
    "    \n",
    "    logging.info(\"Saved Conllu files with metadata to:\")\n",
    "    logging.info(args.output_dir)\n",
    "    \n",
    "    logging.info(f\"Finish.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
