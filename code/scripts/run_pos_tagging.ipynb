{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import stanza\n",
    "import os\n",
    "import argparse\n",
    "from datasets import DatasetDict\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from datasets import list_datasets\n",
    "import logging\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "def get_sentence_doc(sentence_in):\n",
    "    doc = nlp(sentence_in)\n",
    "    return doc\n",
    "\n",
    "def get_postag_token(sentence_in):\n",
    "    ret = []\n",
    "    doc = nlp(sentence_in)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            ret  += [(word.text, word.upos, word.xpos,)]\n",
    "    return ret\n",
    "\n",
    "def partition(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parse():\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='pos-tagging config.')\n",
    "    # Experiment management:\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=128,\n",
    "                        help='Training batch size.')\n",
    "    parser.add_argument('--max_number_of_examples', type=int, default=-1,\n",
    "                        help='Max number of examples to load for each splits.')\n",
    "    parser.add_argument('--data_dir', type=str, default=\"../../data-files/wikitext-15M/\",\n",
    "                        help='Whether to resume for this file.')\n",
    "    parser.add_argument('--output_dir', type=str, default=\"../../data-files/wikitext-15M-pos/\",\n",
    "                        help='Whether to resume for this file.')\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                        help='Random seed.')\n",
    "    parser.set_defaults(\n",
    "        # Exp management:\n",
    "        seed=42,\n",
    "    )\n",
    "    try:\n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        args = parser.parse_args([])\n",
    "    except:\n",
    "        args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Loading arguments\n",
    "    args = arg_parse()\n",
    "    try:        \n",
    "        get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "        # Experiment management:\n",
    "        args.batch_size=128\n",
    "        args.data_dir=\"../../data-files/wikitext-15M/\"\n",
    "        args.output_dir=\"../../data-files/wikitext-15M-pos/\"\n",
    "        args.seed=42\n",
    "        is_jupyter = True\n",
    "    except:\n",
    "        is_jupyter = False\n",
    "    \n",
    "    # Create output directory if not exists.\n",
    "    pathlib.Path(args.output_dir).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, \n",
    "        format='%(asctime)s %(levelname)-8s %(message)s', \n",
    "        datefmt='%a, %d %b %Y %H:%M:%S', \n",
    "        filename=os.path.join(args.output_dir, \"training.log\"),\n",
    "    )\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(os.sys.stdout))\n",
    "    \n",
    "    logging.info(\"Running Pos-tagging with data lives in:\")\n",
    "    logging.info(args.data_dir)\n",
    "    wiki_datasets = DatasetDict.load_from_disk(args.data_dir)\n",
    "    \n",
    "    # Stanza\n",
    "    nlp = stanza.Pipeline('en', processors='tokenize,pos')\n",
    "    logging.info(\"Finish loading Stanza in.\")\n",
    "    \n",
    "    def preprocess(wiki_datasets, args, split):\n",
    "        \n",
    "        logging.info(f\"Preprocessing split={split}.\")\n",
    "        sentences = []\n",
    "        count = 0\n",
    "        for s in wiki_datasets[split]:\n",
    "            sentences += [s[\"text\"]]\n",
    "            count += 1\n",
    "            if count == args.max_number_of_examples:\n",
    "                break\n",
    "        \n",
    "        chunks = list(partition(sentences, args.batch_size))\n",
    "        total_chunk = len(chunks)\n",
    "        count = 0\n",
    "        \n",
    "        sentence_strs = []\n",
    "        upos_strs = []\n",
    "        xpos_strs = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            logging.info(f\"processing: {count+1}/{total_chunk}.\")\n",
    "            in_docs = [stanza.Document([], text=d) for d in chunk]\n",
    "            docs = nlp(in_docs)\n",
    "            for i in range(len(docs)):\n",
    "                sentence_str = []\n",
    "                upos_str = []\n",
    "                xpos_str = []\n",
    "                for sent in docs[i].sentences:\n",
    "                    for word in sent.words:\n",
    "                        sentence_str += [word.text]\n",
    "                        upos_str += [word.upos]\n",
    "                        xpos_str += [word.xpos]\n",
    "                sentence_str = \",\".join(sentence_str)\n",
    "                upos_str = \",\".join(upos_str)\n",
    "                xpos_str = \",\".join(xpos_str)\n",
    "                sentence_strs += [sentence_str]\n",
    "                upos_strs += [upos_str]\n",
    "                xpos_strs += [xpos_str]\n",
    "            count += 1\n",
    "        examples = {\n",
    "            \"sentence_str\":sentence_strs,\n",
    "            \"upos_str\":upos_strs,\n",
    "            \"xpos_str\":xpos_strs,\n",
    "        }\n",
    "        return examples\n",
    "    \n",
    "    examples_train = Dataset.from_dict(preprocess(wiki_datasets, args, \"train\"))\n",
    "    examples_validation = Dataset.from_dict(preprocess(wiki_datasets, args, \"validation\"))\n",
    "    examples_test = Dataset.from_dict(preprocess(wiki_datasets, args, \"test\"))\n",
    "    \n",
    "    dataset_examples = DatasetDict({\n",
    "        \"train\": examples_train,\n",
    "        \"validation\": examples_validation,\n",
    "        \"test\": examples_test,\n",
    "    })\n",
    "    dataset = Dataset.from_dict(dataset_examples)\n",
    "    logging.info(\"Saving Pos-tagging with data to:\")\n",
    "    logging.info(args.output_dir)\n",
    "    dataset.save_to_disk(args.output_dir)\n",
    "    total_count = len(examples_train) + len(examples_validation) + len(examples_test)\n",
    "    logging.info(f\"Collected in total {total_count} examples.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
