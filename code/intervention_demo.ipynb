{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-27 00:27:44 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "2021-07-27 00:27:44 INFO: Use device: gpu\n",
      "2021-07-27 00:27:44 INFO: Loading: tokenize\n",
      "2021-07-27 00:27:56 INFO: Loading: pos\n",
      "2021-07-27 00:28:10 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import stanza\n",
    "from stanza.utils.conll import CoNLL\n",
    "# stanza.download('en')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from datasets import DatasetDict\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from datasets import list_datasets\n",
    "\n",
    "# Utils\n",
    "def get_sentence_doc(sentence_in):\n",
    "    doc = nlp(sentence_in)\n",
    "    return doc\n",
    "\n",
    "def get_postag_token(sentence_in):\n",
    "    ret = []\n",
    "    doc = nlp(sentence_in)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            ret  += [(word.text, word.upos, word.xpos,)]\n",
    "    return ret\n",
    "\n",
    "# Stanza\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo for different aspects we studied with mid-tuning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = 'Every move Google makes brings this particular future closer .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tokenization Differences\n",
    "\n",
    "Loading RoBERTa model with different tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbcd31b7f1f4b518cce28f8722abfe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77593935075414b832149d83275fac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f05fd5864714b68b6c1add6d073aceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=29.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50bb62d38b924a64a8478b235db0dd3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435797.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d7bb0401e8444c9415e4930fcc6436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=481.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5052b50baf44f7b2bdae270cfd5c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=898823.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789894eb5c54471586283a36519dc156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1aba3bc810b4d639375cc48db7ecb34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355863.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a532e5f5f28a47bc906317caca6b3a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769a07ceda34429a8415892e416cba3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798011.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f617ddcefca540a0a42d5b90cd78b8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1382015.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "wordpeice_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../huggingface_cache\"\n",
    ")\n",
    "\n",
    "bpe_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../huggingface_cache\"\n",
    ")\n",
    "\n",
    "sentpeice_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'xlnet-base-cased',\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../huggingface_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = \" The ability to compositionally map language to referents, relations, and actions is an essential component of language understanding. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'ability',\n",
       " 'to',\n",
       " 'composition',\n",
       " '##ally',\n",
       " 'map',\n",
       " 'language',\n",
       " 'to',\n",
       " 'refer',\n",
       " '##ents',\n",
       " ',',\n",
       " 'relations',\n",
       " ',',\n",
       " 'and',\n",
       " 'actions',\n",
       " 'is',\n",
       " 'an',\n",
       " 'essential',\n",
       " 'component',\n",
       " 'of',\n",
       " 'language',\n",
       " 'understanding',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpeice_tokenizer.tokenize(original_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ĠThe',\n",
       " 'Ġability',\n",
       " 'Ġto',\n",
       " 'Ġcomposition',\n",
       " 'ally',\n",
       " 'Ġmap',\n",
       " 'Ġlanguage',\n",
       " 'Ġto',\n",
       " 'Ġrefere',\n",
       " 'nt',\n",
       " 's',\n",
       " ',',\n",
       " 'Ġrelations',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'Ġactions',\n",
       " 'Ġis',\n",
       " 'Ġan',\n",
       " 'Ġessential',\n",
       " 'Ġcomponent',\n",
       " 'Ġof',\n",
       " 'Ġlanguage',\n",
       " 'Ġunderstanding',\n",
       " '.',\n",
       " 'Ġ']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.tokenize(original_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Every',\n",
       " '▁move',\n",
       " '▁Google',\n",
       " '▁makes',\n",
       " '▁brings',\n",
       " '▁this',\n",
       " '▁particular',\n",
       " '▁future',\n",
       " '▁closer',\n",
       " '▁',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentpeice_tokenizer.tokenize(original_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Semantics Shifts\n",
    "\n",
    "Synonym Shift - Nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch annotation using Stanza\n",
    "documents = [\"\", \"I wrote another document for fun.\"]\n",
    "in_docs = [stanza.Document([], text=d) for d in documents]\n",
    "doc = nlp(in_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[0].sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postags = get_postag_token(original_sentence)\n",
    "shifted_sentence = []\n",
    "for p in postags:\n",
    "    if p[-1] == \"NN\":\n",
    "        shifted = False\n",
    "        syns = wordnet.synsets(p[0])\n",
    "        for syn in syns:\n",
    "            shift_w = syn.lemmas()[0].name()\n",
    "            if p[0] != shift_w:\n",
    "                shifted_sentence += [shift_w]\n",
    "                shifted = True\n",
    "                break\n",
    "        if not shifted:\n",
    "            shifted_sentence += [p[0]]\n",
    "    else:\n",
    "        shifted_sentence += [p[0]]\n",
    "\" \".join(shifted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrambling Shift - Nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_identity_map = {\n",
    "    'Google' : 'Facebook',\n",
    "    'move' : 'book',\n",
    "    'future' : 'internet'\n",
    "}\n",
    "shifted_sentence = []\n",
    "for p in postags:\n",
    "    if p[-1] == \"NN\" and p[0] in word_identity_map.keys():\n",
    "        shifted_sentence += [word_identity_map[p[0]]]\n",
    "    else:\n",
    "        shifted_sentence += [p[0]]\n",
    "\" \".join(shifted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concept Merging and Splitting - 1. random merging and random splitting with -X format.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synonym Shift - Nouns: Exploring (1) word embedddings, (2) wordnet nbrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Relevant Shift: FastText Nbrs.\n",
    "* Get all nouns in the wikitext dataset.\n",
    "* Get fasttext embeddings of those nouns.\n",
    "* We need to get the lemma of those words (i.e., \"books\" cannot be swapped with \"book\" since they are considered as the same word lemma).\n",
    "* Using 1-NN matching algorithm to pair up words to swap meanings. We will have some rules to break ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "ft = fasttext.load_model('./data-files/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.get_nearest_neighbors('book', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_datasets = DatasetDict.load_from_disk(\"./data-files/wikitext-15M/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_nouns = set([])\n",
    "count = 0\n",
    "total_count = len(wiki_datasets[\"train\"])\n",
    "for sentence in wiki_datasets[\"train\"]:\n",
    "    if count % 1000 == 0:\n",
    "        print(f\"completed:{count}/{total_count}\")\n",
    "    postags = get_postag_token(sentence['text'])\n",
    "    for p in postags:\n",
    "        if p[-1] == 'NN':\n",
    "            collected_nouns.add(p[0])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Dependency Shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Galatic Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Stanza to get a conllu file for a sentence.\n",
    "sent_doc = get_sentence_doc(original_sentence)\n",
    "CoNLL.write_doc2conll(sent_doc, \"./data-files/sample.conllu\")\n",
    "\n",
    "# runing the command to get galactic dependency.\n",
    "! GALACTIC_ROOT=./submodules/gdtreebank/ ./submodules/gdtreebank/bin/gd-translate --input ./data-files/sample.conllu --spec en~fr@N~hi@V\n",
    "\n",
    "# getting the synthetic sentence.\n",
    "to_sent_doc = CoNLL.conll2doc(\"./data-files/sample-en~fr@N~hi@V.conllu\")\n",
    "\" \".join([item.text for item in to_sent_doc.sentences[0].words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_datasets = DatasetDict.load_from_disk(\"../data-files/wikitext-15M/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4358"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wiki_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the synthetic sentence.\n",
    "to_sent_doc = CoNLL.conll2doc(\"../data-files/wikitext-15M-conllu/wikitext-15M-test.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'= Robert Boulter ='"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([item.text for item in to_sent_doc.sentences[0].words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"id\": 1,\n",
      "    \"text\": \"=\",\n",
      "    \"upos\": \"PUNCT\",\n",
      "    \"xpos\": \"NFP\",\n",
      "    \"head\": 0,\n",
      "    \"misc\": \"\",\n",
      "    \"start_char\": 0,\n",
      "    \"end_char\": 1\n",
      "  },\n",
      "  {\n",
      "    \"id\": 2,\n",
      "    \"text\": \"Robert\",\n",
      "    \"upos\": \"PROPN\",\n",
      "    \"xpos\": \"NNP\",\n",
      "    \"feats\": \"Number=Sing\",\n",
      "    \"head\": 1,\n",
      "    \"misc\": \"\",\n",
      "    \"start_char\": 2,\n",
      "    \"end_char\": 8\n",
      "  },\n",
      "  {\n",
      "    \"id\": 3,\n",
      "    \"text\": \"Boulter\",\n",
      "    \"upos\": \"PROPN\",\n",
      "    \"xpos\": \"NNP\",\n",
      "    \"feats\": \"Number=Sing\",\n",
      "    \"head\": 2,\n",
      "    \"misc\": \"\",\n",
      "    \"start_char\": 9,\n",
      "    \"end_char\": 16\n",
      "  },\n",
      "  {\n",
      "    \"id\": 4,\n",
      "    \"text\": \"=\",\n",
      "    \"upos\": \"PUNCT\",\n",
      "    \"xpos\": \",\",\n",
      "    \"head\": 3,\n",
      "    \"misc\": \"\",\n",
      "    \"start_char\": 17,\n",
      "    \"end_char\": 18\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "for s in to_sent_doc.sentences:\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_sent_doc.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'stanza' has no attribute 'Sentence'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-5664e22bab59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hhhh\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'stanza' has no attribute 'Sentence'"
     ]
    }
   ],
   "source": [
    "stanza.Document(sentences=[stanza.Sentence(text=\"hhhh\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
