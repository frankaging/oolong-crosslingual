{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import stanza\n",
    "from stanza.utils.conll import CoNLL\n",
    "# stanza.download('en')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-chinese\", use_fast=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from datasets import list_datasets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Utils\n",
    "def get_sentence_doc(sentence_in):\n",
    "    doc = nlp(sentence_in)\n",
    "    return doc\n",
    "\n",
    "def get_postag_token(sentence_in):\n",
    "    ret = []\n",
    "    doc = nlp(sentence_in)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            ret  += [(word.text, word.upos, word.xpos,)]\n",
    "    return ret\n",
    "\n",
    "# Stanza\n",
    "# nlp = stanza.Pipeline('en', processors='tokenize,pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo for different aspects we studied with mid-tuning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = 'Every move Google makes brings this particular future closer .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tokenization Differences\n",
    "\n",
    "Loading RoBERTa model with different tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpeice_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../huggingface_cache\"\n",
    ")\n",
    "\n",
    "bpe_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../huggingface_cache\"\n",
    ")\n",
    "\n",
    "sentpeice_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'xlnet-base-cased',\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../huggingface_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = \" The ability to compositionally map language to referents, relations, and actions is an essential component of language understanding. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpeice_tokenizer.tokenize(original_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer.tokenize(original_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentpeice_tokenizer.tokenize(original_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Semantics Shifts\n",
    "\n",
    "Synonym Shift - Nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch annotation using Stanza\n",
    "documents = [\"\", \"I wrote another document for fun.\"]\n",
    "in_docs = [stanza.Document([], text=d) for d in documents]\n",
    "doc = nlp(in_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[0].sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postags = get_postag_token(original_sentence)\n",
    "shifted_sentence = []\n",
    "for p in postags:\n",
    "    if p[-1] == \"NN\":\n",
    "        shifted = False\n",
    "        syns = wordnet.synsets(p[0])\n",
    "        for syn in syns:\n",
    "            shift_w = syn.lemmas()[0].name()\n",
    "            if p[0] != shift_w:\n",
    "                shifted_sentence += [shift_w]\n",
    "                shifted = True\n",
    "                break\n",
    "        if not shifted:\n",
    "            shifted_sentence += [p[0]]\n",
    "    else:\n",
    "        shifted_sentence += [p[0]]\n",
    "\" \".join(shifted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrambling Shift - Nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_identity_map = {\n",
    "    'Google' : 'Facebook',\n",
    "    'move' : 'book',\n",
    "    'future' : 'internet'\n",
    "}\n",
    "shifted_sentence = []\n",
    "for p in postags:\n",
    "    if p[-1] == \"NN\" and p[0] in word_identity_map.keys():\n",
    "        shifted_sentence += [word_identity_map[p[0]]]\n",
    "    else:\n",
    "        shifted_sentence += [p[0]]\n",
    "\" \".join(shifted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concept Merging and Splitting - 1. random merging and random splitting with -X format.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synonym Shift - Nouns: Exploring (1) word embedddings, (2) wordnet nbrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Relevant Shift: FastText Nbrs.\n",
    "* Get all nouns in the wikitext dataset.\n",
    "* Get fasttext embeddings of those nouns.\n",
    "* We need to get the lemma of those words (i.e., \"books\" cannot be swapped with \"book\" since they are considered as the same word lemma).\n",
    "* Using 1-NN matching algorithm to pair up words to swap meanings. We will have some rules to break ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "ft = fasttext.load_model('./data-files/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.get_nearest_neighbors('book', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_datasets = DatasetDict.load_from_disk(\"./data-files/wikitext-15M/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_nouns = set([])\n",
    "count = 0\n",
    "total_count = len(wiki_datasets[\"train\"])\n",
    "for sentence in wiki_datasets[\"train\"]:\n",
    "    if count % 1000 == 0:\n",
    "        print(f\"completed:{count}/{total_count}\")\n",
    "    postags = get_postag_token(sentence['text'])\n",
    "    for p in postags:\n",
    "        if p[-1] == 'NN':\n",
    "            collected_nouns.add(p[0])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Dependency Shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conllu Files Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-in sentences from different files.\n",
    "wiki_datasets = DatasetDict.load_from_disk(\"../data-files/wikitext-15M/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = wiki_datasets[\"test\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(s[\"text\"].strip()) > 0:\n",
    "    clean_s = []\n",
    "    for t in s[\"text\"].strip().split(\" \"):\n",
    "        if len(t.strip()) > 0:\n",
    "            clean_s += [t.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \" \".join(clean_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_docs = [stanza.Document([], text=s)]\n",
    "docs = nlp(in_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs[0].sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoNLL.write_doc2conll(docs[0], \"./test.conllu\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Galatic Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-in sentences from different files.\n",
    "wiki_datasets = DatasetDict.load_from_disk(\"../data-files/wikitext-15M/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_datasets_fr_fr = DatasetDict.load_from_disk(\"../data-files/wikitext-15M-en~fr@N~fr@V/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_datasets_ja_ja = DatasetDict.load_from_disk(\"../data-files/wikitext-15M-en~ja_ktc@N~ja_ktc@V/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_datasets_fr_ja = DatasetDict.load_from_disk(\"../data-files/wikitext-15M-en~fr@N~ja_ktc@V/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_datasets[\"test\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_datasets_fr_fr[\"test\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_datasets = DatasetDict.load_from_disk(\"../data-files/sst3-en~ja_ktc@N~ja_ktc@V/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_datasets[\"validation\"][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_datasets[\"train\"][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-in sentences from different files.\n",
    "sst2_datasets = DatasetDict.load_from_disk(\"../data-files/sst2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_datasets_var_1 = DatasetDict.load_from_disk(\"../data-files/sst2-en~fr@N~fr@V/\")\n",
    "sst2_datasets_var_2 = DatasetDict.load_from_disk(\"../data-files/sst2-en~jaktc@N~jaktc@V/\")\n",
    "sst2_datasets_var_3 = DatasetDict.load_from_disk(\"../data-files/sst2-en~fr@N~jaktc@V/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in sst2_datasets[\"train\"]:\n",
    "    words = example[\"sentence\"].split(\" \")\n",
    "    pickout = \"one of the best films of the year with its exquisite acting , inventive screenplay , mesmerizing music , and many inimitable scenes of tenderness , loss , discontent , and yearning . \"\n",
    "    pickout_words = pickout.split(\" \")\n",
    "\n",
    "    count = 0\n",
    "    for w in pickout_words:\n",
    "        if w in words:\n",
    "            count += 1\n",
    "    if count >= len(pickout_words)*0.3:\n",
    "        print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_datasets[\"train\"][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_datasets_var_1[\"train\"][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_datasets_var_2[\"train\"][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst2_datasets_var_3[\"train\"][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "li = sst2_datasets[\"train\"][idx][\"sentence\"].split(\" \")\n",
    "random.shuffle(li)\n",
    "\" \".join(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = sst2_datasets[\"train\"][idx][\"sentence\"].split(\" \")\n",
    "\" \".join(li[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"GroNLP/bert-base-dutch-cased\"\n",
    "# \"roberta-base\"\n",
    "# \"bert-base-uncased\"\n",
    "# \"albert-base-v2\"\n",
    "# \"flaubert/flaubert_base_cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"GroNLP/bert-base-dutch-cased\",\n",
    "    cache_dir=\"./huggingface_inoculation_cache/\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(tokenizer.tokenize(sst2_datasets[\"train\"][idx][\"sentence\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, copy\n",
    "from vocab_mismatch_utils import *\n",
    "token_frequency_map = json.load(open(\"../data-files/wikitext-15M-vocab.json\"))\n",
    "wikitext_vocab = list(set(token_frequency_map.keys()))\n",
    "# sort so we have consistent map.\n",
    "wikitext_vocab.sort()\n",
    "wikitext_vocab_copy = copy.deepcopy(wikitext_vocab)\n",
    "random.Random(42).shuffle(wikitext_vocab_copy)\n",
    "word_swap_map = {}\n",
    "for i in range(len(wikitext_vocab)):\n",
    "    word_swap_map[wikitext_vocab[i]] = wikitext_vocab_copy[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_basic_tokenizer = ModifiedBasicTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_translator(sst2_datasets[\"train\"][idx][\"sentence\"], modified_basic_tokenizer, word_swap_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    cache_dir=\"./huggingface_inoculation_cache/\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_li = list(tokenizer.get_vocab().keys())\n",
    "random.shuffle(token_li)\n",
    "original_token_li = list(tokenizer.get_vocab().keys())\n",
    "token_swap_map = {}\n",
    "for i in range(len(original_token_li)):\n",
    "    token_swap_map[original_token_li[i]] = token_li[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join([token_swap_map[t] for t in tokenizer.tokenize(sst2_datasets[\"train\"][idx][\"sentence\"])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer Vocab Overlapping Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read-in sentences from different files.\n",
    "sst2_datasets = DatasetDict.load_from_disk(\"../data-files/sst2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dutch_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"GroNLP/bert-base-dutch-cased\",\n",
    "    cache_dir=\"./huggingface_inoculation_cache/\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flaubert_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"flaubert/flaubert_base_cased\",\n",
    "    cache_dir=\"./huggingface_inoculation_cache/\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    cache_dir=\"./huggingface_inoculation_cache/\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "albert_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"albert-base-v2\",\n",
    "    cache_dir=\"./huggingface_inoculation_cache/\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    cache_dir=\"./huggingface_inoculation_cache/\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dutch_lens = []\n",
    "flaubert_lens = []\n",
    "bert_lens = []\n",
    "albert_lens = []\n",
    "roberta_lens = []\n",
    "for example in sst2_datasets[\"train\"]:\n",
    "    dutch_lens += [len(dutch_tokenizer.tokenize(example[\"sentence\"]))]\n",
    "    flaubert_lens += [len(flaubert_tokenizer.tokenize(example[\"sentence\"]))]\n",
    "    bert_lens += [len(bert_tokenizer.tokenize(example[\"sentence\"]))]\n",
    "    albert_lens += [len(albert_tokenizer.tokenize(example[\"sentence\"]))]\n",
    "    roberta_lens += [len(roberta_tokenizer.tokenize(example[\"sentence\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.family\"] = \"DejaVu Serif\"\n",
    "font = {'family' : 'DejaVu Serif',\n",
    "        'size'   : 12}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "with plt.rc_context({'axes.edgecolor':'black', 'xtick.color':'black', 'ytick.color':'black', 'figure.facecolor':'white'}):\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 2.5))\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "    ax.set_title('Sequence Lengths', fontsize=20)\n",
    "    ax.boxplot(\n",
    "        [roberta_lens, bert_lens, albert_lens, dutch_lens, flaubert_lens], widths = 0.3,\n",
    "        showfliers=False,\n",
    "        boxprops=dict(color='#117733',linewidth=2),\n",
    "        medianprops=dict(color='#117733',linewidth=2),\n",
    "        capprops=dict(linewidth=2, color='#117733'),\n",
    "        whiskerprops=dict(linewidth=2,linestyle='--', color='#117733')\n",
    "    )\n",
    "    plt.xticks([1, 2, 3, 4, 5], [\"RoBERTa\", \"BERT\", \"Albert\", \"FlauBERT\", \"DutchBERT\"], fontsize=12)\n",
    "    ax.spines[\"top\"].set_linewidth(2)\n",
    "    ax.spines[\"bottom\"].set_linewidth(2)\n",
    "    ax.spines[\"left\"].set_linewidth(2)\n",
    "    ax.spines[\"right\"].set_linewidth(2)\n",
    "    ax.spines[\"top\"].set_linewidth(2)\n",
    "    ax.spines[\"bottom\"].set_linewidth(2)\n",
    "    ax.spines[\"left\"].set_linewidth(2)\n",
    "    ax.spines[\"right\"].set_linewidth(2)\n",
    "    ax.xaxis.grid(color='grey', linestyle='-.', linewidth=1, alpha=0.5)\n",
    "    ax.yaxis.grid(color='grey', linestyle='-.', linewidth=1, alpha=0.5)\n",
    "    ax.set_ylabel('Lengths', fontsize=20)\n",
    "    # plt.show()\n",
    "    plt.savefig(\"../data-files/tokenizer-seq-len.png\",dpi=1000, bbox_inches='tight')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(flaubert_lens)/len(flaubert_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(roberta_lens)/len(roberta_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(15.096853702356382-12.359604448469911)/12.359604448469911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"microsoft/deberta-v3-base\",\n",
    "    cache_dir=\"./huggingface_inoculation_cache/\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
