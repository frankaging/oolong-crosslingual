{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import stanza\n",
    "from stanza.utils.conll import CoNLL\n",
    "# stanza.download('en')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from datasets import DatasetDict\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from datasets import list_datasets\n",
    "\n",
    "# Utils\n",
    "def get_sentence_doc(sentence_in):\n",
    "    doc = nlp(sentence_in)\n",
    "    return doc\n",
    "\n",
    "def get_postag_token(sentence_in):\n",
    "    ret = []\n",
    "    doc = nlp(sentence_in)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            ret  += [(word.text, word.upos, word.xpos,)]\n",
    "    return ret\n",
    "\n",
    "# Stanza\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo for different aspects we studied with mid-tuning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = 'Every move Google makes brings this particular future closer .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tokenization Differences\n",
    "\n",
    "Loading RoBERTa model with different tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpeice_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../huggingface_cache\"\n",
    ")\n",
    "\n",
    "bpe_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../huggingface_cache\"\n",
    ")\n",
    "\n",
    "sentpeice_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'xlnet-base-cased',\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../huggingface_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = \" The ability to compositionally map language to referents, relations, and actions is an essential component of language understanding. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpeice_tokenizer.tokenize(original_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer.tokenize(original_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentpeice_tokenizer.tokenize(original_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Semantics Shifts\n",
    "\n",
    "Synonym Shift - Nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch annotation using Stanza\n",
    "documents = [\"\", \"I wrote another document for fun.\"]\n",
    "in_docs = [stanza.Document([], text=d) for d in documents]\n",
    "doc = nlp(in_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[0].sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postags = get_postag_token(original_sentence)\n",
    "shifted_sentence = []\n",
    "for p in postags:\n",
    "    if p[-1] == \"NN\":\n",
    "        shifted = False\n",
    "        syns = wordnet.synsets(p[0])\n",
    "        for syn in syns:\n",
    "            shift_w = syn.lemmas()[0].name()\n",
    "            if p[0] != shift_w:\n",
    "                shifted_sentence += [shift_w]\n",
    "                shifted = True\n",
    "                break\n",
    "        if not shifted:\n",
    "            shifted_sentence += [p[0]]\n",
    "    else:\n",
    "        shifted_sentence += [p[0]]\n",
    "\" \".join(shifted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrambling Shift - Nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_identity_map = {\n",
    "    'Google' : 'Facebook',\n",
    "    'move' : 'book',\n",
    "    'future' : 'internet'\n",
    "}\n",
    "shifted_sentence = []\n",
    "for p in postags:\n",
    "    if p[-1] == \"NN\" and p[0] in word_identity_map.keys():\n",
    "        shifted_sentence += [word_identity_map[p[0]]]\n",
    "    else:\n",
    "        shifted_sentence += [p[0]]\n",
    "\" \".join(shifted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concept Merging and Splitting - 1. random merging and random splitting with -X format.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synonym Shift - Nouns: Exploring (1) word embedddings, (2) wordnet nbrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Relevant Shift: FastText Nbrs.\n",
    "* Get all nouns in the wikitext dataset.\n",
    "* Get fasttext embeddings of those nouns.\n",
    "* We need to get the lemma of those words (i.e., \"books\" cannot be swapped with \"book\" since they are considered as the same word lemma).\n",
    "* Using 1-NN matching algorithm to pair up words to swap meanings. We will have some rules to break ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "ft = fasttext.load_model('./data-files/cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.get_nearest_neighbors('book', k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_datasets = DatasetDict.load_from_disk(\"./data-files/wikitext-15M/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_nouns = set([])\n",
    "count = 0\n",
    "total_count = len(wiki_datasets[\"train\"])\n",
    "for sentence in wiki_datasets[\"train\"]:\n",
    "    if count % 1000 == 0:\n",
    "        print(f\"completed:{count}/{total_count}\")\n",
    "    postags = get_postag_token(sentence['text'])\n",
    "    for p in postags:\n",
    "        if p[-1] == 'NN':\n",
    "            collected_nouns.add(p[0])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Dependency Shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Galatic Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Stanza to get a conllu file for a sentence.\n",
    "sent_doc = get_sentence_doc(original_sentence)\n",
    "CoNLL.write_doc2conll(sent_doc, \"./data-files/sample.conllu\")\n",
    "\n",
    "# runing the command to get galactic dependency.\n",
    "! GALACTIC_ROOT=./submodules/gdtreebank/ ./submodules/gdtreebank/bin/gd-translate --input ./data-files/sample.conllu --spec en~fr@N~hi@V\n",
    "\n",
    "# getting the synthetic sentence.\n",
    "to_sent_doc = CoNLL.conll2doc(\"./data-files/sample-en~fr@N~hi@V.conllu\")\n",
    "\" \".join([item.text for item in to_sent_doc.sentences[0].words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_datasets = DatasetDict.load_from_disk(\"../data-files/wikitext-15M/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wiki_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the synthetic sentence.\n",
    "to_sent_doc = CoNLL.conll2doc(\"../data-files/wikitext-15M-conllu/wikitext-15M-test.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join([item.text for item in to_sent_doc.sentences[0].words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in to_sent_doc.sentences:\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(to_sent_doc.sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.Document(sentences=[stanza.Sentence(text=\"hhhh\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
