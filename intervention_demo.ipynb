{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-14 01:59:56 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2021-07-14 01:59:56 INFO: Use device: cpu\n",
      "2021-07-14 01:59:56 INFO: Loading: tokenize\n",
      "2021-07-14 01:59:56 INFO: Loading: pos\n",
      "2021-07-14 01:59:56 INFO: Loading: lemma\n",
      "2021-07-14 01:59:56 INFO: Loading: depparse\n",
      "2021-07-14 01:59:58 INFO: Loading: sentiment\n",
      "2021-07-14 01:59:58 INFO: Loading: ner\n",
      "2021-07-14 01:59:59 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import stanza\n",
    "from stanza.utils.conll import CoNLL\n",
    "# stanza.download('en')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "# Utils\n",
    "def get_sentence_doc(sentence_in):\n",
    "    doc = nlp(sentence_in)\n",
    "    return doc\n",
    "\n",
    "def get_postag_token(sentence_in):\n",
    "    ret = []\n",
    "    doc = nlp(sentence_in)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            ret  += [(word.text, word.upos, word.xpos,)]\n",
    "    return ret\n",
    "\n",
    "# Stanza\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demo for different aspects we studied with mid-tuning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentence = 'Every move Google makes brings this particular future closer .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tokenization Differences\n",
    "\n",
    "Loading RoBERTa model with different tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpeice_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../huggingface_cache\"\n",
    ")\n",
    "\n",
    "bpe_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"roberta-base\",\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../huggingface_cache\"\n",
    ")\n",
    "\n",
    "sentpeice_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'xlnet-base-cased',\n",
    "    use_fast=False,\n",
    "    cache_dir=\"../huggingface_cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Every',\n",
       " 'move',\n",
       " 'Google',\n",
       " 'makes',\n",
       " 'brings',\n",
       " 'this',\n",
       " 'particular',\n",
       " 'future',\n",
       " 'closer',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpeice_tokenizer.tokenize(original_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Every',\n",
       " 'Ġmove',\n",
       " 'ĠGoogle',\n",
       " 'Ġmakes',\n",
       " 'Ġbrings',\n",
       " 'Ġthis',\n",
       " 'Ġparticular',\n",
       " 'Ġfuture',\n",
       " 'Ġcloser',\n",
       " 'Ġ.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.tokenize(original_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Every',\n",
       " '▁move',\n",
       " '▁Google',\n",
       " '▁makes',\n",
       " '▁brings',\n",
       " '▁this',\n",
       " '▁particular',\n",
       " '▁future',\n",
       " '▁closer',\n",
       " '▁',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentpeice_tokenizer.tokenize(original_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Semantics Shifts\n",
    "\n",
    "Synonym Shift - Nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every motion Google makes brings this particular future closer .'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postags = get_postag_token(original_sentence)\n",
    "shifted_sentence = []\n",
    "for p in postags:\n",
    "    if p[-1] == \"NN\":\n",
    "        shifted = False\n",
    "        syns = wordnet.synsets(p[0])\n",
    "        for syn in syns:\n",
    "            shift_w = syn.lemmas()[0].name()\n",
    "            if p[0] != shift_w:\n",
    "                shifted_sentence += [shift_w]\n",
    "                shifted = True\n",
    "                break\n",
    "        if not shifted:\n",
    "            shifted_sentence += [p[0]]\n",
    "    else:\n",
    "        shifted_sentence += [p[0]]\n",
    "\" \".join(shifted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrambling Shift - Nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Every book Google makes brings this particular internet closer .'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_identity_map = {\n",
    "    'Google' : 'Facebook',\n",
    "    'move' : 'book',\n",
    "    'future' : 'internet'\n",
    "}\n",
    "shifted_sentence = []\n",
    "for p in postags:\n",
    "    if p[-1] == \"NN\" and p[0] in word_identity_map.keys():\n",
    "        shifted_sentence += [word_identity_map[p[0]]]\n",
    "    else:\n",
    "        shifted_sentence += [p[0]]\n",
    "\" \".join(shifted_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concept Merging and Splitting - Nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synonym Shift - Nouns: Exploring (1) word embedddings, (2) wordnet nbrs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Dependency Shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Galatic Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java -cp ./submodules/gdtreebank//bin/gdgen.jar -Xmx8g datagen.GalacticGen --task test --inputTB ./data-files/sample.conllu --outputTB ./data-files/sample-en~fr@N~hi@V.conllu --verbose 1 --seed 0 --supStrateModelNOUN ./submodules/gdtreebank//models/GD_French/fr@N.orm --subStrateModelNOUN ./submodules/gdtreebank//models/GD_English/en@N.orm --supStrateModelVERB ./submodules/gdtreebank//models/GD_Hindi/hi@V.orm --subStrateModelVERB ./submodules/gdtreebank//models/GD_English/en@V.orm\n",
      "0        INFO                GalacticGen - Running with args: --task test --inputTB ./data-files/sample.conllu --outputTB ./data-files/sample-en~fr@N~hi@V.conllu --verbose 1 --seed 0 --supStrateModelNOUN ./submodules/gdtreebank//models/GD_French/fr@N.orm --subStrateModelNOUN ./submodules/gdtreebank//models/GD_English/en@N.orm --supStrateModelVERB ./submodules/gdtreebank//models/GD_Hindi/hi@V.orm --subStrateModelVERB ./submodules/gdtreebank//models/GD_English/en@V.orm\n",
      "WARNING: pseudo random number generator is not thread safe\n",
      "SEED=123456789101112\n",
      "SEED=0\n",
      "57 [main] INFO edu.jhu.pacaya.util.Threads - Initialized default thread pool to 10 threads. (This must be closed by Threads.shutdownDefaultPool)\n",
      "432      INFO                GalacticGen - permutable Nodes:VERB NOUN\n",
      "439      INFO                   TreeBank - Loading proposal grammar from Standard TreeBank:sample.conllu\n",
      "511      INFO                   TreeBank - 1/1 are valid trees\n",
      "511      INFO                   TreeBank - Generate new TreeBank from Standard TreeBank by permutation\n",
      "[|||||||||||||||||||||||||||||||||||||||||||||||||||]\n",
      "554      INFO                   TreeBank - 1/1 are valid trees\n",
      "550 [main] INFO edu.jhu.pacaya.util.Threads - Attempting shutdown of ExecutorService.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'this particular future Every move Google makes closer brings .'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Stanza to get a conllu file for a sentence.\n",
    "sent_doc = get_sentence_doc(original_sentence)\n",
    "CoNLL.write_doc2conll(sent_doc, \"./data-files/sample.conllu\")\n",
    "\n",
    "# runing the command to get galactic dependency.\n",
    "! GALACTIC_ROOT=./submodules/gdtreebank/ ./submodules/gdtreebank/bin/gd-translate --input ./data-files/sample.conllu --spec en~fr@N~hi@V\n",
    "\n",
    "# getting the synthetic sentence.\n",
    "to_sent_doc = CoNLL.conll2doc(\"./data-files/sample-en~fr@N~hi@V.conllu\")\n",
    "\" \".join([item.text for item in to_sent_doc.sentences[0].words])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
